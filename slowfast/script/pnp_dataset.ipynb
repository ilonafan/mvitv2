{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172d5620-8172-4060-9345-48be39bcd6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvhci/temp/prgc/anaconda3/envs/env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_cuda.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d64c53ce-45fd-4eab-8671-26ae9f89c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5e89d3-6899-4d12-9cb3-523481a5b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transform(rescale_size=512, crop_size=448):\n",
    "    train_transform = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize(size=rescale_size),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        # torchvision.transforms.RandomCrop(size=crop_size),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    test_transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(size=rescale_size),\n",
    "        torchvision.transforms.CenterCrop(size=crop_size),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    train_transform_strong_aug = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(size=rescale_size),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomCrop(size=crop_size),\n",
    "        # RandAugment(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    return {'train': train_transform, 'test': test_transform, 'train_strong_aug': train_transform_strong_aug}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c1ed02-fe23-4e09-b9b1-3cc8c0ac3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6729f41d-3efc-4936-abf4-445f6e192a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return 20\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # sample = np.ones((1, 2, 3, 4))\n",
    "        imarray = np.random.rand(600,600,3) * 255\n",
    "        sample = Image.fromarray(imarray.astype('uint8')).convert('RGB')\n",
    "        label = \"This is label\"\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        sample = {'data': sample, 'label': label, 'idx': idx}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ef494d-fb9a-467d-8950-968655b096bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLDataTransform(object):\n",
    "    def __init__(self, transform_weak, transform_strong):\n",
    "        self.transform_weak = transform_weak\n",
    "        self.transform_strong = transform_strong\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        x_w1 = self.transform_weak(sample)\n",
    "        x_w2 = self.transform_weak(sample)\n",
    "        x_s = self.transform_strong(sample)\n",
    "        return x_w1, x_w2, x_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17851888-f6a8-4444-8d16-1c49c38c3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(mode, transform):\n",
    "    if mode == 'train':\n",
    "        dataset = CustomDataset('train', transform=transform)\n",
    "    elif mode == 'val' or mode == 'test':\n",
    "        dataset = CustomDataset('test', transform=transform)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71538bf9-c0bf-48bf-876f-1aba027c2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_loader():\n",
    "    transform = build_transform()\n",
    "    train_dataset = build_dataset('train', CLDataTransform(transform['train'], transform['train_strong_aug']))\n",
    "    test_dataset = build_dataset('test', transform['test'])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=False)\n",
    "    dataset = {'train': train_dataset, 'test': test_dataset}\n",
    "    return dataset, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "238bfd18-1d8b-4646-a69c-83f963984c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, train_loader, test_loader = build_dataset_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f27bc48-03da-4fba-bb74-c993aec9e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b62bd43-7ef2-46d8-a8b9-5732301990ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of x is:  torch.Size([16, 3, 600, 600])\n",
      "The type of x is:  torch.Size([3, 600, 600])\n",
      "The type of x_w is:  torch.Size([3, 600, 600])\n",
      "The type of x_s is:  torch.Size([3, 448, 448])\n",
      "The label is:  This is label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(train_loader, ncols=150, ascii=' >', leave=False, desc='training')\n",
    "for it, sample in enumerate(pbar):\n",
    "    index = sample['idx']\n",
    "    label = sample['label']\n",
    "    x, x_w, x_s = sample['data']\n",
    "    assert type(sample['data']) == list and len(sample['data']) == 3\n",
    "    print(\"The type of x is: \", sample['data'][0].shape)\n",
    "    print(\"The type of x is: \", x[0].shape)\n",
    "    print(\"The type of x_w is: \", x_w[0].shape)\n",
    "    print(\"The type of x_s is: \", x_s[0].shape)\n",
    "    print(\"The label is: \", label[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928c3aef-50a4-4db9-b2ec-788de4593586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "69617f05-823a-4c9e-b15e-e103b9cfcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.Tensor([[0, 0,0,1,0,0], [0, 0,0,0,0,1], [0, 1,0,0,0,0], [0, 0,0,0,0,1], [0, 1,0,0,0,0], [0, 0,0,0,0,1], [0, 0,1,0,0,0], [0, 0,1,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f5d2198-9d68-4c00-a0db-a6e1556bd48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3ed493-4468-4997-b830-03a3d218fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((2, 3, 16, 224, 224))\n",
    "x_mo = torch.zeros((1, 25088, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c27d04-c5b8-4e93-bd41-6eea467921d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_prob = torch.randn((4, 10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b7d570-77f6-45b5-bd99-ed840ec209a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = type_prob.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "864d73a5-a57b-463f-a4b6-0acc0894d6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33a88ec2-714c-4eb9-9f28-1785ff93f766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = type_prob.view(type_prob.shape[0], -1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a182a0cc-dc30-4bd9-a1e3-c2b5bc54f325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.softmax(dim=1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fba4325-83cf-41cc-b92d-e033e4ef0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_target = torch.nn.functional.one_hot(type_prob.max(dim=1)[1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eecc6799-8674-4ced-b6e9-4e7ec0aeebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_target(target, num_classes, lam=1.0, smoothing=0.0):\n",
    "    \"\"\"\n",
    "    This function converts target class indices to one-hot vectors, given the\n",
    "    number of classes.\n",
    "    Args:\n",
    "        targets (loader): Class labels.\n",
    "        num_classes (int): Total number of classes.\n",
    "        lam (float): lamba value for mixup/cutmix.\n",
    "        smoothing (float): Label smoothing value.\n",
    "    \"\"\"\n",
    "    off_value = smoothing / num_classes\n",
    "    on_value = 1.0 - smoothing + off_value\n",
    "    target1 = convert_to_one_hot(\n",
    "        target,\n",
    "        num_classes,\n",
    "        on_value=on_value,\n",
    "        off_value=off_value,\n",
    "    )\n",
    "    target2 = convert_to_one_hot(\n",
    "        target.flip(0),\n",
    "        num_classes,\n",
    "        on_value=on_value,\n",
    "        off_value=off_value,\n",
    "    )\n",
    "    return target1 * lam + target2 * (1.0 - lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4530a09f-3e5c-4caf-8fd5-f2b1e8938ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(targets, num_classes, on_value=1.0, off_value=0.0):\n",
    "    \"\"\"\n",
    "    This function converts target class indices to one-hot vectors, given the\n",
    "    number of classes.\n",
    "    Args:\n",
    "        targets (loader): Class labels.\n",
    "        num_classes (int): Total number of classes.\n",
    "        on_value (float): Target Value for ground truth class.\n",
    "        off_value (float): Target Value for other classes.This value is used for\n",
    "            label smoothing.\n",
    "    \"\"\"\n",
    "\n",
    "    targets = targets.long().view(-1, 1)\n",
    "    return torch.full(\n",
    "        (targets.size()[0], num_classes), off_value, device=targets.device\n",
    "    ).scatter_(1, targets, on_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2168dd78-8934-4e49-9bb7-6e042fd8562a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8fd14ae0-b5fd-4703-9eb9-5f4e7259e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = mixup_target(labels, 6, smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "64395feb-963b-4f99-bdcd-91a5d9702132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 6])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c23121a-667f-4bbd-8b19-d2ece4b93a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7fa7366-55cd-441b-b4a2-5be8aebf5504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6932,  0.3799,  2.0800, -0.8293, -0.1116, -0.4470],\n",
       "        [ 0.1443,  0.9974,  0.8769, -0.8658,  0.0143, -0.6859],\n",
       "        [-0.3839,  0.1288, -1.4223,  1.7675, -1.2043, -0.6170],\n",
       "        [ 0.4345, -0.4345,  1.5307,  0.3862, -0.1115,  0.2062]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((4,6))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5214050b-2ba9-4684-91ad-3b4b9a07762c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1488, 0.1088, 0.5957, 0.0325, 0.0666, 0.0476],\n",
       "        [0.1407, 0.3303, 0.2928, 0.0513, 0.1236, 0.0614],\n",
       "        [0.0778, 0.1299, 0.0275, 0.6689, 0.0343, 0.0616],\n",
       "        [0.1484, 0.0622, 0.4440, 0.1414, 0.0859, 0.1181]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.softmax(dim=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8880f82a-2c56-41d5-bc33-63824db7c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[0,0,0,1,0,0], [0,1,0,0,0,0], [0,1,0,0,0,0], [1,0,0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6f87233-4afc-4963-af8f-1c5e7c366b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88cf775e-d826-4f7a-b0c4-4be48ed1ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smoothed_label_distribution(labels, num_class, epsilon):\n",
    "    smoothed_label = torch.full(size=(labels.size(0), num_class), fill_value=epsilon / (num_class))\n",
    "    addier = torch.mul(labels.cpu(), 1 - epsilon)\n",
    "    smoothed_label = torch.add(smoothed_label, addier)\n",
    "    # smoothed_label.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1).cpu(), value=1 - epsilon)\n",
    "    return smoothed_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3086c7b5-229e-4e55-a788-583903c9e77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.1000, 0.1000, 0.5000, 0.1000, 0.1000],\n",
       "        [0.1000, 0.5000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
       "        [0.1000, 0.5000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
       "        [0.5000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothed = get_smoothed_label_distribution(a, 6, 0.6)\n",
    "smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bb0fa2-423d-44a7-9128-d73c9655c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div(p, q, base=2):\n",
    "    # p, q is in shape (batch_size, n_classes)\n",
    "    if base == 2:\n",
    "        return (p * p.log2() - p * q.log2()).sum(dim=1)\n",
    "    else:\n",
    "        return (p * p.log() - p * q.log()).sum(dim=1)\n",
    "\n",
    "\n",
    "def symmetric_kl_div(p, q, base=2):\n",
    "    return kl_div(p, q, base) + kl_div(q, p, base)\n",
    "\n",
    "\n",
    "def js_div(p, q, base=2):\n",
    "    # Jensen-Shannon divergence, value is in (0, 1)\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * kl_div(p, m, base) + 0.5 * kl_div(q, m, base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b9ef853-aefb-4bf7-8c49-c332f7e95e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_probs(probs, given_labels): \n",
    "    return (1 - js_div(probs, given_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1623f5d1-74ee-4eb1-9ed1-963acd53dc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6678, 0.9392, 0.7140, 0.8359])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = clean_probs(z, get_smoothed_label_distribution(a, 6, 0.6))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8946ea26-8741-44c7-b73c-e8fe295c4172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5712, 0.4096, 0.8021, 0.3492])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = clean_probs(z, get_smoothed_label_distribution(a, 6, 0.1))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04a0d91c-884d-43f9-859c-e3b777b24b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4355, 0.2667, 0.6857, 0.1941])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = clean_probs(z, get_smoothed_label_distribution(a, 6, 0.01))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9df005d-7367-4865-9af6-3c0e04cc25f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93779bf8-23e5-4feb-a56a-f43314dce66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df205af2-e17a-4bbc-99e6-b5228d4fdea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360a1158-bc65-4774-a28d-eb8b012a0665",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand((2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea97e8ea-5de3-4f69-9820-9fceaef07291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0184, 0.6524, 0.1635, 0.0159],\n",
       "        [0.2061, 0.3924, 0.4875, 0.5115]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17bf8837-d722-41c1-83ae-4368ad94532d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (torch.abs(x * y) >= thresh).type(torch.FloatTensor)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a158809-df8e-4078-acbb-4d9d0793d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9d88ad3-31ac-4adc-a685-0e2363cb154e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8, 1. ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_narry = np.linspace(1-0.2, 1, num=2)\n",
    "clip_narry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52ac9abb-5ce2-4637-9e8c-992459ec389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_narry = clip_narry[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99ecdae3-d1b2-4488-9f6d-35afdaefda31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.98947368, 0.97894737, 0.96842105, 0.95789474,\n",
       "       0.94736842, 0.93684211, 0.92631579, 0.91578947, 0.90526316,\n",
       "       0.89473684, 0.88421053, 0.87368421, 0.86315789, 0.85263158,\n",
       "       0.84210526, 0.83157895, 0.82105263, 0.81052632, 0.8       ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_narry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3e7b245-29c4-4d7a-970a-2e1fea8ba787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = round(26 * 0.1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81aa3c5-ba33-4e8b-824b-11194db33ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
